{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb05472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, argparse\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate.utils import fsdp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f07683",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bd15788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "\n",
    "def extract_numerical_answer(answer_text):\n",
    "    # GSM8K answers end with #### followed by the numerical answer\n",
    "    match = re.search(r\"#### ([-\\d,]+)\", answer_text)\n",
    "    if match:\n",
    "        # Remove commas and convert to int\n",
    "        return int(match.group(1).replace(\",\", \"\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2b99fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    #print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db94d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/data2/alex/verifiers/outputs/Qwen-1.5B-GRPO-base/checkpoint-1869'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e40064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c57d94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af59c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"openai/gsm8k\", \"main\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b853e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for i, item in enumerate(data):\n",
    "    proccessed = {\n",
    "        \"question\": item[\"question\"],\n",
    "        \"prompt\": SYSTEM_PROMPT + \" \" + item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"numerical_answer\": extract_numerical_answer(item[\"answer\"]),\n",
    "        \"other_answer\": extract_hash_answer(item[\"answer\"]),\n",
    "    }\n",
    "    eval_data.append(proccessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b27b3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [item[\"prompt\"] for item in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/11 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating:   0%|          | 0/11 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "batch_size = 128\n",
    "for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating\"):\n",
    "    batch_prompt = prompts[i:i+batch_size]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=786,\n",
    "    ).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input,\n",
    "            max_new_tokens=786,\n",
    "            temperature = 0.0,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    for j, outputs in enumerate(outputs):\n",
    "        generated_text = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "        prompt = batch_prompt[j]\n",
    "        generated_texts.append(generated_text.replace(prompt[j], '')})\n",
    "        #print(f\"Prompt {i+j}: {prompt[j]}\")\n",
    "        #print('-' * 160)\n",
    "        #print(f\"Generated Text: {generated_text.replace(prompt[j], '')}\")\n",
    "        #print(\"-\" * 160)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f031a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  198, 65354,   304,   279,  2701,  3561,   510,    27, 19895,   287,\n",
       "           397,  9338,   522, 19895,   287,   397,    27,  9217,   397,  9338,\n",
       "           522,  9217,   397, 17599,   323,   220,    18,   315,   806,  4780,\n",
       "          1973,   220,    22, 87770,   369, 15786,    13,  8886, 22502,   374,\n",
       "          3931,  1119,   220,    23, 34254,    13,  1416, 17599,   323,   806,\n",
       "          4780,  1366,   311,  4332,   279, 87770, 18308,    11,  1246,  1657,\n",
       "         34254,   646,  1817,   315,  1105,   614,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93daa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}